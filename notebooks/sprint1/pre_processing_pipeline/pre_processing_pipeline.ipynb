{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQtsPgODg9zu"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel -q\n",
        "!pip install -U spacy -q\n",
        "!python -m spacy download en_core_web_sm -q"
      ],
      "metadata": {
        "id": "Hv1dSL0Qd6l-",
        "outputId": "9aac3a9f-abe4-4018-ee43-76367aa3973d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbkTeTn57GUF",
        "outputId": "136811f3-6971-4bf0-fa2b-234ef55429bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from enum import Enum, auto\n",
        "from typing import List, Type, Union\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import RSLPStemmer\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.xx import MultiLanguage\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGVERaoMhAjY"
      },
      "source": [
        "# Defining PreprocessingTextPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8vXTk2Yz_iDZ"
      },
      "outputs": [],
      "source": [
        "class NLPLibrary(Enum):\n",
        "    nlkt = 0\n",
        "    spacy = 1\n",
        "\n",
        "class TextProcessingStep(ABC):\n",
        "    @abstractmethod\n",
        "    def execute(self, data: Union[str, List[str]]) -> Union[str, List[str]]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def requires(self) -> List[Type['TextProcessingStep']]:\n",
        "        pass\n",
        "\n",
        "class PreprocessingTextPipeline():\n",
        "    def __init__(self, nlpLibrary: NLPLibrary):\n",
        "        self.steps = []\n",
        "        self.nlpLibrary = nlpLibrary\n",
        "\n",
        "    def add_step(self, step: TextProcessingStep):\n",
        "        for required_step in step.requires():\n",
        "            if not any(isinstance(s, required_step) for s in self.steps):\n",
        "                raise ValueError(f\"Step {step.__class__.__name__} requires {required_step.__name__} to be added first.\")\n",
        "\n",
        "        self.steps.append(step)\n",
        "\n",
        "    def run(self, data: Union[str, List[str]]) -> Union[str, List[str]]:\n",
        "        for step in self.steps:\n",
        "            data = step.execute(data, self.nlpLibrary)\n",
        "        return data\n",
        "\n",
        "class Lowercase(TextProcessingStep):\n",
        "    def execute(self, data: str, library: NLPLibrary) -> str:\n",
        "        return data.lower()\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return []\n",
        "\n",
        "class RemoveNumbers(TextProcessingStep):\n",
        "    def execute(self, data: str, library: NLPLibrary) -> str:\n",
        "        return re.sub(r'\\d+', '', data)\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return []\n",
        "\n",
        "class RemovePunctuation(TextProcessingStep):\n",
        "    def execute(self, data: str, library: NLPLibrary) -> str:\n",
        "        translator = str.maketrans('', '', string.punctuation)\n",
        "        return data.translate(translator)\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return []\n",
        "\n",
        "class Tokenization(TextProcessingStep):\n",
        "    def execute(self, data: str, library: NLPLibrary) -> List[str]:\n",
        "        if library == 0:\n",
        "            return word_tokenize(data)\n",
        "        elif library == 1:\n",
        "            tokens = MultiLanguage().tokenizer(data)\n",
        "            return [token.text for token in tokens]\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return []\n",
        "\n",
        "class RemoveStopwords(TextProcessingStep):\n",
        "    def execute(self, data: List[str], library: NLPLibrary) -> List[str]:\n",
        "        if library == 0:\n",
        "            stop_words = set(stopwords.words('portuguese'))\n",
        "            return [word for word in data if word.lower() not in stop_words]\n",
        "        else:\n",
        "            raise NotImplementedError(\"Implement spaCy way\")\n",
        "            return []\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return [Tokenization]\n",
        "\n",
        "class Stemming(TextProcessingStep):\n",
        "    def execute(self, data: List[str], library: NLPLibrary) -> List[str]:\n",
        "        if library == 0:\n",
        "            stemmer = RSLPStemmer()\n",
        "            return [stemmer.stem(word) for word in data]\n",
        "        else:\n",
        "            raise NotImplementedError(\"Implement spaCy way\")\n",
        "            return []\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return [Tokenization, RemoveStopwords]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhymu92i28ou"
      },
      "source": [
        "#### Defining pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H4a1TU3YhNpG"
      },
      "outputs": [],
      "source": [
        "pipeline = PreprocessingTextPipeline(0)\n",
        "\n",
        "pipeline.add_step(Lowercase())\n",
        "pipeline.add_step(RemoveNumbers())\n",
        "pipeline.add_step(RemovePunctuation())\n",
        "pipeline.add_step(Tokenization())\n",
        "pipeline.add_step(RemoveStopwords())\n",
        "pipeline.add_step(Stemming())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qJHYUkUI82W_"
      },
      "outputs": [],
      "source": [
        "raw_text = \"Bom dia! Meu nome é Jonas Brothers e eu sou do grupo 1.\"\n",
        "preprocessed_text = pipeline.run(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQXy2zyC89Pq",
        "outputId": "beee1f23-ec88-4ee2-bb53-aa5df175f461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bom', 'dia', 'nom', 'jon', 'broth', 'grup']\n"
          ]
        }
      ],
      "source": [
        "print(preprocessed_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}